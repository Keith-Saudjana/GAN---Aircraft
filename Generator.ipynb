{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik6TowcZDhA5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2befXggIc8m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/university/introduction to AI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtKzlX7AINo6"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX181eW7IRbQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# imports keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Permute, Reshape\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, LeakyReLU, ZeroPadding2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D, Conv2DTranspose, UpSampling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "from tensorflow.keras.layers import Flatten, Input\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhJT1GVTK64Y"
      },
      "source": [
        "# Global Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiLCrRiHLOfe"
      },
      "outputs": [],
      "source": [
        "img_shape = (64, 64)\n",
        "tf.keras.utils.set_random_seed(1)\n",
        "batch_size = 64\n",
        "latent_dim = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amm1h1B8IzQ8"
      },
      "source": [
        "# Preprocess The Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH68XOVL9csn"
      },
      "source": [
        "## Load the bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JpXjYkyIAM0"
      },
      "outputs": [],
      "source": [
        "IMAGES_BOX_PATH = 'fgvc-aircraft-2013b/data/images_box.txt'\n",
        "# dictionary of img_name as key and all other infos as values\n",
        "image_infos = {}\n",
        "with open(IMAGES_BOX_PATH) as f:\n",
        "  for line in f.readlines():\n",
        "    img_name, xmin, ymin, xmax, ymax = line.rstrip('\\n').split(' ')\n",
        "    image_infos[img_name] = {'xmin': int(xmin), 'ymin': int(ymin), 'xmax': int(xmax), 'ymax': int(ymax), 'tensor': np.array([])}\n",
        "\n",
        "next(iter(image_infos.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLy361uc9jzj"
      },
      "source": [
        "## Load the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t41DA28g-esz"
      },
      "outputs": [],
      "source": [
        "IMAGES_PATH = 'fgvc-aircraft-2013b/data/images'\n",
        "\n",
        "for idx, filename in tqdm(enumerate(os.listdir(IMAGES_PATH))):\n",
        "  # plt.figure()\n",
        "  img = cv2.imread(os.path.join(IMAGES_PATH, filename))\n",
        "  img_name = filename.split('.')[0]\n",
        "  xmin = image_infos[img_name]['xmin']\n",
        "  ymin = image_infos[img_name]['ymin']\n",
        "  xmax = image_infos[img_name]['xmax']\n",
        "  ymax = image_infos[img_name]['ymax']\n",
        "  \n",
        "  cropped_img = img[ymin: ymax, xmin: xmax]\n",
        "  cropped_img = cv2.resize(cropped_img, img_shape)\n",
        "  \n",
        "  image_infos[img_name]['tensor'] = np.array(cropped_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unn0uhuLCYPk"
      },
      "source": [
        "## Load train, val, test data for family, manufacturer, variant into data structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7tslBIPDUgP"
      },
      "outputs": [],
      "source": [
        "IMAGES_FAMILY_PATH_TRAIN = 'fgvc-aircraft-2013b/data/images_family_train.txt'\n",
        "IMAGES_FAMILY_PATH_VAL = 'fgvc-aircraft-2013b/data/images_family_val.txt'\n",
        "IMAGES_FAMILY_PATH_TEST = 'fgvc-aircraft-2013b/data/images_family_test.txt'\n",
        "\n",
        "IMAGES_MANUFACTURER_PATH_TRAIN = 'fgvc-aircraft-2013b/data/images_manufacturer_train.txt'\n",
        "IMAGES_MANUFACTURER_PATH_VAL = 'fgvc-aircraft-2013b/data/images_manufacturer_val.txt'\n",
        "IMAGES_MANUFACTURER_PATH_TEST = 'fgvc-aircraft-2013b/data/images_manufacturer_test.txt'\n",
        "\n",
        "IMAGES_VARIANT_PATH_TRAIN = 'fgvc-aircraft-2013b/data/images_variant_train.txt'\n",
        "IMAGES_VARIANT_PATH_VAL = 'fgvc-aircraft-2013b/data/images_variant_val.txt'\n",
        "IMAGES_VARIANT_PATH_TEST = 'fgvc-aircraft-2013b/data/images_variant_test.txt'\n",
        "\n",
        "IMAGES_PATH_TRAIN = 'fgvc-aircraft-2013b/data/images_train.txt'\n",
        "IMAGES_PATH_VAL = 'fgvc-aircraft-2013b/data/images_val.txt'\n",
        "IMAGES_PATH_TEST = 'fgvc-aircraft-2013b/data/images_test.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvO1m4mhmjiK"
      },
      "outputs": [],
      "source": [
        "def load_spec(path, data_type = 'train', spec = 'family'):\n",
        "  \"\"\"\n",
        "    Loads the family data and store it to a dictionary of a dictionary (defined in global) containing information of the data\n",
        "\n",
        "    input\n",
        "      path : the path for the dataset information in txt\n",
        "      data_type : a string value of either 'train', 'val', or 'test'\n",
        "      spec : a string value of either 'family', 'manufacturer' or 'variant'\n",
        "  \n",
        "  \"\"\"\n",
        "  with open(path) as f:\n",
        "    for line in f.readlines():\n",
        "      line_list = line.rstrip('\\n').split(' ')\n",
        "      # raise exception if line_list has only len = 1\n",
        "      img_name, spec_data  = line_list[0], ' '.join(line_list[i] for i in range(1, len(line_list)))\n",
        "      # adds new element in images_infos dictionary\n",
        "      image_infos[img_name][spec] = spec_data\n",
        "      image_infos[img_name]['data_type'] = data_type\n",
        "\n",
        "# load family train, val, test\n",
        "load_spec(IMAGES_FAMILY_PATH_TRAIN, 'train', 'family')\n",
        "load_spec(IMAGES_FAMILY_PATH_VAL, 'val', 'family')\n",
        "load_spec(IMAGES_FAMILY_PATH_TEST, 'test', 'family')\n",
        "# load manufacturer train, val, test\n",
        "load_spec(IMAGES_MANUFACTURER_PATH_TRAIN, 'train', 'manufacturer')\n",
        "load_spec(IMAGES_MANUFACTURER_PATH_VAL, 'val', 'manufacturer')\n",
        "load_spec(IMAGES_MANUFACTURER_PATH_TEST, 'test', 'manufacturer')\n",
        "# load variant train, val, test\n",
        "load_spec(IMAGES_VARIANT_PATH_TRAIN, 'train', 'variant')\n",
        "load_spec(IMAGES_VARIANT_PATH_VAL, 'val', 'variant')\n",
        "load_spec(IMAGES_VARIANT_PATH_TEST, 'test', 'variant')\n",
        "\n",
        "next(iter(image_infos.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCc-M5O-1KHZ"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGmQYlER-D9Z"
      },
      "source": [
        "### Preprocess X data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vim8bsPN2iEd"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "\n",
        "with open(IMAGES_PATH_TRAIN) as f:\n",
        "  for line in f.readlines():\n",
        "    img_name = line.rstrip('\\n')\n",
        "    if (np.any(image_infos[img_name]['tensor'])):\n",
        "      X_train.append(image_infos[img_name]['tensor'])\n",
        "\n",
        "with open(IMAGES_PATH_VAL) as f:\n",
        "  for line in f.readlines():\n",
        "    img_name = line.rstrip('\\n')\n",
        "    if (np.any(image_infos[img_name]['tensor'])):\n",
        "      X_train.append(image_infos[img_name]['tensor'])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_train = (X_train - 127.5) / 127.5\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvcfHy2bFbQA"
      },
      "source": [
        "# Plot the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWz2f9yRFd3F"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i, (key, val) in enumerate(image_infos.items()):\n",
        "  if i == 25: \n",
        "    break\n",
        "  ax = plt.subplot(5, 5, i + 1)\n",
        "  if (np.any(val['tensor'])):\n",
        "    plt.imshow(val['tensor'].astype(\"uint8\"))\n",
        "  plt.title(key)\n",
        "  plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi9qRJ7SJNe2"
      },
      "source": [
        "# Build Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cirVI2l5QoKv"
      },
      "source": [
        "## Generator Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8jL6ltiJK_p"
      },
      "outputs": [],
      "source": [
        "def define_generator():\n",
        "  inputs = Input(shape = (latent_dim, ))\n",
        "  # foundation for 4x4 image\n",
        "  block1 = Dense(256 * 4 * 4, use_bias = False)(inputs)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  block1 = Reshape((4, 4, 256))(block1)\n",
        "  # upsample to 8x8\n",
        "  block1 = UpSampling2D((2, 2))(block1)\n",
        "  block1 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1,1), padding='same', use_bias = False)(block1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "\n",
        "  # upsample to 16x16\n",
        "  block1 = UpSampling2D((2, 2))(block1)\n",
        "  block1 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1), padding='same', use_bias = False)(block1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  # upsample to 32x32\n",
        "  block1 = UpSampling2D((2, 2))(block1)\n",
        "  block1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), padding='same', use_bias = False)(block1)\n",
        "  block1 = BatchNormalization()(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  \n",
        "  # upsample to 64x64\n",
        "  block1 = UpSampling2D((2, 2))(block1)\n",
        "  outputs = Conv2D(filters=3, kernel_size=(3,3), activation='tanh', padding='same', use_bias = False)(block1)\n",
        "  outputs = BatchNormalization()(outputs)\n",
        "  return Model(inputs = inputs, outputs = outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgRIMTkbQlS9"
      },
      "outputs": [],
      "source": [
        "gen_model = define_generator()\n",
        "gen_model.summary()\n",
        "tf.keras.utils.plot_model(gen_model, show_shapes=True, show_layer_names=True, dpi=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjK5MK3sS5vI"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = gen_model(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpk8SqiJSW2L"
      },
      "source": [
        "## Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XokIMGHSaki"
      },
      "outputs": [],
      "source": [
        "def define_discriminator():\n",
        "  inputs = Input(shape=(img_shape[0], img_shape[1], 3))\n",
        "  # assert(in_shape == (128, 128, 3))\n",
        "\n",
        "  x = ZeroPadding2D((2, 2))(inputs)\n",
        "  # Hidden Layer 1\n",
        "  block1 = Conv2D(filters=64, kernel_size=(5,5), strides=(2, 2), padding='same')(inputs)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  \n",
        "  # Hidden Layer 2\n",
        "  block1 = Conv2D(filters=128, kernel_size=(5,5), strides=(2, 2), padding='same')(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  block1 = Dropout(0.3)(block1)\n",
        "  \n",
        "  # Hidden Layer 3\n",
        "  block1 = Conv2D(filters=256, kernel_size=(5,5), strides=(2, 2), padding='same')(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  block1 = Dropout(0.3)(block1)\n",
        "\n",
        "  block1 = Conv2D(filters=512, kernel_size=(5,5), strides=(2, 2), padding='same')(block1)\n",
        "  block1 = LeakyReLU(alpha=0.2)(block1)\n",
        "  block1 = Dropout(0.3)(block1)\n",
        "  \n",
        "  # Flatten and Output Layers\n",
        "  block1 = Flatten()(block1) # Flatten the shape\n",
        "  block1 = Dropout(0.2)(block1) # Randomly drop some connections for better generalization\n",
        "\n",
        "  # outputs layer\n",
        "  outputs = Dense(1)(block1) # Output Layer\n",
        "  \n",
        "  model = Model(inputs = inputs, outputs = outputs, name = 'discriminator')\n",
        "\n",
        "  return model\n",
        "\n",
        "# Instantiate\n",
        "dis_model = define_discriminator()\n",
        "\n",
        "# Show model summary and plot model diagram\n",
        "dis_model.summary()\n",
        "tf.keras.utils.plot_model(dis_model, show_shapes=True, show_layer_names=True, dpi=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0VQnMw5WO_Z"
      },
      "outputs": [],
      "source": [
        "decision = dis_model(generated_image)\n",
        "print(decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXbQwMBNZF4u"
      },
      "source": [
        "## GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-pYDebOZLI_"
      },
      "outputs": [],
      "source": [
        "class WGAN(Model):\n",
        "  def __init__(\n",
        "    self,\n",
        "    discriminator,\n",
        "    generator,\n",
        "    latent_dim,\n",
        "    discriminator_extra_steps=3,\n",
        "    gp_weight=10.0,\n",
        "  ):\n",
        "    super(WGAN, self).__init__()\n",
        "    self.discriminator = discriminator\n",
        "    self.generator = generator\n",
        "    self.latent_dim = latent_dim\n",
        "    self.d_steps = discriminator_extra_steps\n",
        "    self.gp_weight = gp_weight\n",
        "\n",
        "  def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
        "    super(WGAN, self).compile()\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.d_loss_fn = d_loss_fn\n",
        "    self.g_loss_fn = g_loss_fn\n",
        "\n",
        "  def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "    \"\"\" Calculates the gradient penalty.\n",
        "\n",
        "    This loss is calculated on an interpolated image\n",
        "    and added to the discriminator loss.\n",
        "    \"\"\"\n",
        "    alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "    diff = fake_images - real_images\n",
        "    interpolated = real_images + alpha * diff\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "      gp_tape.watch(interpolated)\n",
        "      pred = self.discriminator(interpolated, training=True)\n",
        "      grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "      norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "      gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "      return gp\n",
        "\n",
        "  def train_step(self, real_images):\n",
        "    if isinstance(real_images, tuple):\n",
        "      real_images = real_images[0]\n",
        "\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "    for i in tqdm(range(self.d_steps)):\n",
        "      random_latent_vectors = tf.random.normal(\n",
        "        shape=(batch_size, self.latent_dim)\n",
        "      )\n",
        "      with tf.GradientTape() as tape:\n",
        "        fake_images = self.generator(random_latent_vectors, training=True)\n",
        "        fake_logits = self.discriminator(fake_images, training=True)\n",
        "        real_logits = self.discriminator(real_images, training=True)\n",
        "        d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
        "        gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "        d_loss = d_cost + gp * self.gp_weight\n",
        "        d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "          zip(d_gradient, self.discriminator.trainable_variables)\n",
        "        )\n",
        "\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "    with tf.GradientTape() as tape:\n",
        "      generated_images = self.generator(random_latent_vectors, training=True)\n",
        "      gen_img_logits = self.discriminator(generated_images, training=True)\n",
        "      g_loss = self.g_loss_fn(gen_img_logits)\n",
        "    gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "    self.g_optimizer.apply_gradients(\n",
        "      zip(gen_gradient, self.generator.trainable_variables)\n",
        "    )\n",
        "    return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWazfgHLuy6y"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, num_img=6, latent_dim=128):\n",
        "    self.num_img = num_img\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "    generated_images = self.model.generator(random_latent_vectors)\n",
        "    generated_images = (generated_images * 127.5) + 127.5\n",
        "    tf.saved_model.save(wgan, \"saved_model/my_model/wgan_data_clean_cp\")\n",
        "\n",
        "    for i in range(self.num_img):\n",
        "      img = generated_images[i].numpy()\n",
        "      img = tf.keras.preprocessing.image.array_to_img(img)\n",
        "      img.save(\"generated_epoch/generated_img_{epoch}.jpg\".format( epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJyUHx4feV6"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Odh7YBfgkK"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "generator_optimizer = Adam(\n",
        "  learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "discriminator_optimizer = Adam(\n",
        "  learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
        ")\n",
        "\n",
        "def discriminator_loss(real_img, fake_img):\n",
        "  real_loss = tf.reduce_mean(real_img)\n",
        "  fake_loss = tf.reduce_mean(fake_img)\n",
        "  return fake_loss - real_loss\n",
        "\n",
        "\n",
        "def generator_loss(fake_img):\n",
        "  return -tf.reduce_mean(fake_img)\n",
        "\n",
        "epochs = 600\n",
        "\n",
        "cbk = GANMonitor(num_img=1, latent_dim=latent_dim)\n",
        "\n",
        "wgan = WGAN(\n",
        "  discriminator=dis_model,\n",
        "  generator=gen_model,\n",
        "  latent_dim=latent_dim,\n",
        "  discriminator_extra_steps=3,\n",
        ")\n",
        "\n",
        "wgan.compile(\n",
        "  d_optimizer=discriminator_optimizer,\n",
        "  g_optimizer=generator_optimizer,\n",
        "  g_loss_fn=generator_loss,\n",
        "  d_loss_fn=discriminator_loss,\n",
        ")\n",
        "\n",
        "wgan.fit(X_train, batch_size=batch_size, epochs=epochs, callbacks=[cbk])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7SIBrKd0ReO"
      },
      "source": [
        "## Generate Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfciz9lf0YKf"
      },
      "source": [
        "### load model and plot image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl_Ut1wBg957"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "trained_model = tf.saved_model.load(\"saved_model/my_model/wgan\")\n",
        "mums_to_show = 20\n",
        "g = trained_model.generator\n",
        "random_latent_vectors = tf.random.normal(shape=(mums_to_show, latent_dim))\n",
        "\n",
        "\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(20,15))\n",
        "gs = fig.add_gridspec(10, 10)\n",
        "\n",
        "for line in range(0,10):\n",
        "  for row in range(0,10):\n",
        "    random_latent_vectors = tf.random.normal(shape=(mums_to_show, latent_dim))\n",
        "    generated_images = g(random_latent_vectors, training=False)\n",
        "    generated_images = (generated_images * 127.5) + 127.5\n",
        "    num_image = generated_images[1].numpy()\n",
        "    num_image = tf.keras.preprocessing.image.array_to_img(num_image)\n",
        "    ax = fig.add_subplot(gs[line, row])\n",
        "    ax.axis('off')\n",
        "    ax.imshow(num_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYIzhszS0bEo"
      },
      "source": [
        "### save to folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KraG6B5UehuS"
      },
      "outputs": [],
      "source": [
        "num_img = 50\n",
        "random_latent_vectors = tf.random.normal(shape=(num_img, latent_dim))\n",
        "generated_images = g(random_latent_vectors)\n",
        "generated_images = (generated_images * 127.5) + 127.5\n",
        "\n",
        "for i in range(1, num_img+1):\n",
        "  random_latent_vectors = tf.random.normal(shape=(num_img, latent_dim))\n",
        "  generated_images = g(random_latent_vectors)\n",
        "  generated_images = (generated_images * 127.5) + 127.5\n",
        "  img = generated_images[i].numpy()\n",
        "  img = tf.keras.preprocessing.image.array_to_img(img)\n",
        "  img.save(\"/content/drive/MyDrive/university/introduction to AI/group17/{i}.jpg\".format(i=i))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SH68XOVL9csn",
        "aLy361uc9jzj",
        "Unn0uhuLCYPk",
        "rGmQYlER-D9Z",
        "ZvcfHy2bFbQA",
        "cirVI2l5QoKv",
        "xpk8SqiJSW2L",
        "QXbQwMBNZF4u"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
